{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missingValCounter(df):\n",
    "    count = (df == -1).astype(int).sum(axis=1)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading files...\n"
     ]
    }
   ],
   "source": [
    "print('loading files...')\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "col_to_drop = train.columns[train.columns.str.startswith('ps_calc_')]\n",
    "low_variance_col = [\"ps_ind_11_bin\", \"ps_ind_13_bin\", \"ps_ind_12_bin\", \"ps_ind_18_bin\", \"ps_car_10_cat\", \n",
    "                    \"ps_car_11_cat\"]\n",
    "train = train.drop(col_to_drop, axis=1)\n",
    "test = test.drop(col_to_drop, axis=1)\n",
    "missValsTrain = missingValCounter(train)\n",
    "missValsTest = missingValCounter(test)\n",
    "\n",
    "train[\"ps_car_12\"] = train[\"ps_car_12\"].apply(lambda x: round(x**2, 4) * 10000)\n",
    "train[\"ps_car_13\"] = train[\"ps_car_13\"].apply(lambda x: round(x**2 * 48400, 2))\n",
    "train[\"ps_car_15\"] = train[\"ps_car_15\"].apply(lambda x: round(x**2))\n",
    "\n",
    "test[\"ps_car_12\"] = test[\"ps_car_12\"].apply(lambda x: round(x**2, 4) * 10000)\n",
    "test[\"ps_car_13\"] = test[\"ps_car_13\"].apply(lambda x: round(x**2 * 48400, 2))\n",
    "test[\"ps_car_15\"] = test[\"ps_car_15\"].apply(lambda x: round(x**2))\n",
    "\n",
    "missValsTrain = missValsTrain.to_frame(\"missingVals\")\n",
    "missValsTest = missValsTest.to_frame(\"missingVals\")\n",
    "\n",
    "train[\"missing_values\"] = missValsTrain[\"missingVals\"].values\n",
    "test[\"missing_values\"] = missValsTest[\"missingVals\"].values\n",
    "\n",
    "#train = train.drop(low_variance_col, axis=1)\n",
    "#test = test.drop(low_variance_col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = train.columns[train.columns.str.endswith(\"_cat\")]\n",
    "cat_col_val = train[cat_cols]\n",
    "cat_col_val = cat_col_val + 1\n",
    "\n",
    "cat_col_test = test.columns[test.columns.str.endswith(\"_cat\")] \n",
    "cat_col_val_test = test[cat_col_test]\n",
    "cat_col_val_test = cat_col_val_test + 1\n",
    "\n",
    "encTrain = OneHotEncoder()\n",
    "#train.drop(cat_cols)\n",
    "encTrain.fit(cat_col_val)\n",
    "oneHotVal = encTrain.transform(cat_col_val).toarray()\n",
    "\n",
    "encTest = OneHotEncoder()\n",
    "#test.drop(cat_col_test)\n",
    "encTest.fit(cat_col_val_test)\n",
    "oneHotValTest = encTest.transform(cat_col_val_test).toarray()\n",
    "\n",
    "test = test.drop(cat_col_test, axis=1)\n",
    "train = train.drop(cat_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features_train = pd.DataFrame(oneHotVal)\n",
    "new_features_train.rename(columns=lambda x: x+1, inplace=True)\n",
    "new_features_train.rename(columns=lambda x: \"feature_\" + str(x), inplace=True)\n",
    "train = pd.concat([train, new_features_train], axis=1)\n",
    "\n",
    "new_features_test = pd.DataFrame(oneHotValTest)\n",
    "new_features_test.rename(columns=lambda x: x+1, inplace=True)\n",
    "new_features_test.rename(columns=lambda x: \"feature_\" + str(x), inplace=True)\n",
    "test = pd.concat([test, new_features_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [i for i in range(1,len(new_features_train.columns))]\n",
    "for i in lst:\n",
    "    new_features_train[str(i)] = new_features_train[\"feature_\" + str(i)].apply(lambda x: int(x))\n",
    "for i in lst:\n",
    "    new_features_test[str(i)] = new_features_test[\"feature_\" + str(i)].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all zeros\n",
    "del train[\"feature_77\"]\n",
    "del test[\"feature_77\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(595212, 209) (892816, 208)\n"
     ]
    }
   ],
   "source": [
    "for c in train.select_dtypes(include=['float64']).columns:\n",
    "    train[c]=train[c].astype(np.float32)\n",
    "    test[c]=test[c].astype(np.float32)\n",
    "for c in train.select_dtypes(include=['int64']).columns[2:]:\n",
    "    train[c]=train[c].astype(np.int8)\n",
    "    test[c]=test[c].astype(np.int8)    \n",
    "\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(y, pred):\n",
    "    g = np.asarray(np.c_[y, pred, np.arange(len(y)) ], dtype=np.float)\n",
    "    g = g[np.lexsort((g[:,2], -1*g[:,1]))]\n",
    "    gs = g[:,0].cumsum().sum() / g[:,0].sum()\n",
    "    gs -= (len(y) + 1) / 2.\n",
    "    return gs / len(y)\n",
    "\n",
    "def gini_xgb(pred, y):\n",
    "    y = y.get_label()\n",
    "    return 'gini', gini(y, pred) / gini(y, y)\n",
    "\n",
    "def gini_lgb(preds, dtrain):\n",
    "    y = list(dtrain.get_label())\n",
    "    score = gini(y, preds) / gini(y, y)\n",
    "    return 'gini', score, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(['id', 'target'], axis=1)\n",
    "features = X.columns\n",
    "X = X.values\n",
    "y = train['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " xgb kfold: 1  of  5 : \n",
      "[0]\ttrain-gini:0.214602\tvalid-gini:0.214817\n",
      "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n",
      "\n",
      "Will train until valid-gini hasn't improved in 100 rounds.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-e5f2b9777abd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mwatchlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     xgb_model = xgb.train(params, d_train, nrounds, watchlist, early_stopping_rounds=100, \n\u001b[0;32m---> 29\u001b[0;31m                           feval=gini_xgb, maximize=True, verbose_eval=100)\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     sub['target'] += xgb_model.predict(xgb.DMatrix(test[features].values), \n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, learning_rates, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m             \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# xgb\n",
    "params = {'eta': 0.01, 'max_depth': 5, 'subsample': 0.8, 'colsample_bytree': 0.9, \n",
    "          'objective': 'binary:logistic', 'eval_metric': 'auc', 'silent': True}\n",
    "\n",
    "\n",
    "sub=test['id'].to_frame()\n",
    "sub['target']=0\n",
    "\n",
    "nrounds=2000\n",
    "kfold = 5\n",
    "\n",
    "skf = StratifiedKFold(n_splits=kfold, random_state=1)\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    print(' xgb kfold: {}  of  {} : '.format(i+1, kfold))\n",
    "    X_train, X_valid = X[train_index], X[test_index]\n",
    "    y_train, y_valid = y[train_index], y[test_index]\n",
    "    d_train = xgb.DMatrix(X_train, y_train) \n",
    "    d_valid = xgb.DMatrix(X_valid, y_valid) \n",
    "    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "    xgb_model = xgb.train(params, d_train, nrounds, watchlist, early_stopping_rounds=100, \n",
    "                          feval=gini_xgb, maximize=True, verbose_eval=100)\n",
    "    \n",
    "    sub['target'] += xgb_model.predict(xgb.DMatrix(test[features].values), \n",
    "                        ntree_limit=xgb_model.best_ntree_limit+50) / (2*kfold)\n",
    "gc.collect()\n",
    "\n",
    "# lgb\n",
    "params = {'metric': 'auc', 'learning_rate' : 0.1, 'max_depth':10, 'max_bin':10,  'objective': 'binary', \n",
    "          'feature_fraction': 0.8,'bagging_fraction':0.9,'bagging_freq':10,  'min_data': 500}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=kfold, random_state=1)\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    print(' lgb kfold: {}  of  {} : '.format(i+1, kfold))\n",
    "    X_train, X_eval = X[train_index], X[test_index]\n",
    "    y_train, y_eval = y[train_index], y[test_index]\n",
    "    lgb_model = lgb.train(params, lgb.Dataset(X_train, label=y_train), nrounds, \n",
    "                  lgb.Dataset(X_eval, label=y_eval), verbose_eval=100, \n",
    "                  feval=gini_lgb, early_stopping_rounds=100)\n",
    "    \n",
    "    sub['target'] += lgb_model.predict(test[features].values, \n",
    "                        num_iteration=lgb_model.best_iteration) / (2*kfold)\n",
    "\n",
    "log_model = LogisticRegression()\n",
    "log_model.fit(train[features].values, y)\n",
    "sub['target'] += log_model.predict_proba(test[features].values)[:,1] / (2*kfold)\n",
    "\n",
    "sub.to_csv('sub10log.csv', index=False, float_format='%.5f') \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(892816, 207)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[features].values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        1.488026e+06\n",
       "target    1.174328e+00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-48-62ab42495b2c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-48-62ab42495b2c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    xgb kfold: 1  of  5 :\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    " xgb kfold: 1  of  5 : \n",
    "[0]\ttrain-gini:0.199908\tvalid-gini:0.194267\n",
    "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n",
    "\n",
    "Will train until valid-gini hasn't improved in 100 rounds.\n",
    "[100]\ttrain-gini:0.260527\tvalid-gini:0.250359\n",
    "[200]\ttrain-gini:0.269496\tvalid-gini:0.253377\n",
    "[300]\ttrain-gini:0.282427\tvalid-gini:0.258978\n",
    "[400]\ttrain-gini:0.297443\tvalid-gini:0.266783\n",
    "[500]\ttrain-gini:0.310735\tvalid-gini:0.271728\n",
    "[600]\ttrain-gini:0.321808\tvalid-gini:0.275316\n",
    "[700]\ttrain-gini:0.331675\tvalid-gini:0.278089\n",
    "[800]\ttrain-gini:0.340027\tvalid-gini:0.279733\n",
    "[900]\ttrain-gini:0.347624\tvalid-gini:0.281073\n",
    "[1000]\ttrain-gini:0.354826\tvalid-gini:0.281934\n",
    "[1100]\ttrain-gini:0.361346\tvalid-gini:0.282563\n",
    "[1200]\ttrain-gini:0.367924\tvalid-gini:0.282746"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
